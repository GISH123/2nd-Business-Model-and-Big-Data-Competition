{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.read_csv('merged.csv') # option: nrows=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make catboost know which features are categorical\n",
    "merged['srno'] = merged['srno'].astype(str)\n",
    "merged['YYYYMM'] = merged['YYYYMM'].astype(str)\n",
    "merged['c_gender'] = merged['c_gender'].astype(str)\n",
    "merged['c_zip'] = merged['c_zip'].astype(str)\n",
    "merged['c_edu'] = merged['c_edu'].astype(str)\n",
    "merged['c_mry'] = merged['c_mry'].astype(str)\n",
    "merged['c_job'] = merged['c_job'].astype(str)\n",
    "merged['c_occp'] = merged['c_occp'].astype(str)\n",
    "merged['a_incm_flg'] = merged['a_incm_flg'].astype(str)\n",
    "merged['x_flg_house'] = merged['x_flg_house'].astype(str)\n",
    "merged['CAR_FLG'] = merged['CAR_FLG'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged['py_total']\n",
    "del merged['as_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = merged[merged['YYYYMM'] != '201812']\n",
    "test = merged[merged['YYYYMM'] == '201812']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['YYYYMM'] # 感覺這個東西不能訓練，如果當類別變數的話，基本上test是不可能跟train有一樣的時間的\n",
    "del test['YYYYMM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['y1','y2'], axis=1)\n",
    "y = train[['y1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_indices = np.where(X.dtypes == object)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.75, random_state=1337,stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'iterations': 200,\n",
    "    'learning_rate': 0.1,\n",
    "    'loss_function':'Logloss', #CrossEntropy?\n",
    "    'eval_metric': 'F1',\n",
    "    'random_seed': 1337,\n",
    "    'logging_level': 'Silent',\n",
    "    'class_weights' : [1, 100]\n",
    "    #'scale_pos_weight' : 10   #(1410132-4786)/4786\n",
    "    #,'use_best_model' : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(X_train, y_train, cat_features=categorical_features_indices)\n",
    "validate_pool = Pool(X_validation, y_validation, cat_features=categorical_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(**params)\n",
    "model.fit(train_pool, eval_set=validate_pool,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#early stop model\n",
    "earlystop_params = params.copy()\n",
    "earlystop_params.update({\n",
    "    'iterations': 10000,\n",
    "    'od_type': 'Iter',\n",
    "    # 'loss_function':'CrossEntropy', #CrossEntropy?\n",
    "    'od_wait': 300,\n",
    "    'class_weights' : None\n",
    "    \n",
    "})\n",
    "earlystop_model = CatBoostClassifier(**earlystop_params)\n",
    "earlystop_model.fit(train_pool, eval_set=validate_pool,plot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看simple model之feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.get_feature_importance(train_pool)\n",
    "feature_names = X_train.columns\n",
    "for score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n",
    "    print('{}: {}'.format(name, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看early stop model之feature importancee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = earlystop_model.get_feature_importance(train_pool)\n",
    "feature_names = X_train.columns\n",
    "for score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n",
    "    print('{}: {}'.format(name, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict proba from simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(['y1','y2'], axis=1)\n",
    "y_test = test[['y1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions_probs = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.85# threshold we set where the probability prediction must be above this to be classified as a '1'\n",
    "classes = predictions_probs[:,1] # say it is the class in the second column you care about predictint\n",
    "classes[classes>=threshold] = 1\n",
    "classes[classes<threshold] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues, occurCount = np.unique(classes, return_counts=True)\n",
    " \n",
    "print(\"Unique Values : \" , uniqueValues)\n",
    "print(\"Occurrence Count : \", occurCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此為輸入成csv的語法\n",
    "# np.savetxt(\"pred_proba(threshold=0.52,model(scr == int),scale pos weight = 10,800 iters,f1 = 0.1258).csv\", classes, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Early stop model predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = earlystop_model.predict(X_test)\n",
    "predictions_probs = earlystop_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6 # threshold we set where the probability prediction must be above this to be classified as a '1'\n",
    "classes = predictions_probs[:,1] # say it is the class in the second column you care about predictint\n",
    "classes[classes>=threshold] = 1\n",
    "classes[classes<threshold] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues, occurCount = np.unique(classes, return_counts=True)\n",
    " \n",
    "print(\"Unique Values : \" , uniqueValues)\n",
    "print(\"Occurrence Count : \", occurCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此為輸入成csv的語法\n",
    "# np.savetxt(\"pred_proba(threshold=0.52,model(scr == int),scale pos weight = 10,800 iters,f1 = 0.1258).csv\", classes, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
